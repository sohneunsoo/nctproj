{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-14 04:53:44.131061: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-14 04:53:45.206283: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-04-14 04:53:45.206402: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-04-14 04:53:45.206414: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import re\n",
    "import gc\n",
    "\n",
    "import tensorflow as tf\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing import sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Physical GPUs, 4 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-14 04:53:50.940171: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:53:50.940630: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:53:50.940965: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:53:50.941313: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:53:50.952801: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:53:50.953204: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:53:50.953537: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:53:50.953859: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:53:50.954195: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:53:50.954531: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:53:50.954907: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:53:50.955264: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:53:50.956865: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-14 04:53:51.344258: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:53:51.344674: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:53:51.344997: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:53:51.345323: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:53:51.345644: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:53:51.345949: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:53:51.346292: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:53:51.346612: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:53:51.346949: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:53:51.347275: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:53:51.347598: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:53:51.347927: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:53:53.343826: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:53:53.344286: I tensorflow/compiler/xla/stream_execu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:53:53.344654: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:53:53.345001: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:53:53.345353: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:53:53.345682: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:53:53.346026: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:53:53.346397: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:53:53.346780: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:53:53.347120: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14622 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n",
      "2023-04-14 04:53:53.347728: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:53:53.348044: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 14622 MB memory:  -> device: 1, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:05.0, compute capability: 7.0\n",
      "2023-04-14 04:53:53.348797: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:53:53.349120: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 14622 MB memory:  -> device: 2, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:06.0, compute capability: 7.0\n",
      "2023-04-14 04:53:53.349455: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:53:53.349797: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 14622 MB memory:  -> device: 3, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:07.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n",
      "/job:localhost/replica:0/task:0/device:GPU:1 -> device: 1, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:05.0, compute capability: 7.0\n",
      "/job:localhost/replica:0/task:0/device:GPU:2 -> device: 2, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:06.0, compute capability: 7.0\n",
      "/job:localhost/replica:0/task:0/device:GPU:3 -> device: 3, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:07.0, compute capability: 7.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-14 04:54:13.136907: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:54:13.137405: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:54:13.137776: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:54:13.138188: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:54:13.138563: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:54:13.138943: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:54:13.139318: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:54:13.139658: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:54:13.140018: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:54:13.140350: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:54:13.140675: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:54:13.141000: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:54:13.141550: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:54:13.141931: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:54:13.142302: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:54:13.142672: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:54:13.143084: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:54:13.143406: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14622 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n",
      "2023-04-14 04:54:13.143495: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:54:13.143799: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 14622 MB memory:  -> device: 1, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:05.0, compute capability: 7.0\n",
      "2023-04-14 04:54:13.143870: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:54:13.144215: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 14622 MB memory:  -> device: 2, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:06.0, compute capability: 7.0\n",
      "2023-04-14 04:54:13.144283: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 04:54:13.144567: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 14622 MB memory:  -> device: 3, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:07.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.compat.v1.keras.backend import set_session\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.python.client import device_lib\n",
    "# device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train.csv', header=None, names=['text','time','label'])\n",
    "test = pd.read_csv('./data/test.csv', header=None, names=['text','time','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_list = train['text'].tolist()\n",
    "X_test_list = test['text'].tolist()\n",
    "y_train = train['label'].tolist()\n",
    "y_test = test['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train \n",
    "del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 'allenai/longformer-base-4096'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer(X_train_list, truncation=True, padding=True)\n",
    "# X_test = tokenizer(X_test, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('X_train.pkl','rb') as f:\n",
    "    X_train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train_list\n",
    "del X_test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(X_train['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(X_train),\n",
    "    y_train\n",
    "))\n",
    "\n",
    "# val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "#     dict(X_test),\n",
    "#     y_test\n",
    "# ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"NG\", 1: \"Groomer\"}\n",
    "label2id = {\"NG\": 0, \"Groomer\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers.legacy import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at allenai/longformer-base-4096 were not used when initializing TFLongformerForSequenceClassification: ['lm_head']\n",
      "- This IS expected if you are initializing TFLongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFLongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFLongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "tf.debugging.set_log_device_placement(True)\n",
    "gpus = tf.config.list_logical_devices('GPU')\n",
    "strategy = tf.distribute.MirroredStrategy(gpus)\n",
    "with strategy.scope():\n",
    "    model = TFAutoModelForSequenceClassification.from_pretrained(M, num_labels=2, id2label=id2label, label2id=label2id)\n",
    "    optimizer = Adam(learning_rate=0.0001) #learning_rate=5e-5\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-14 01:59:59.002910: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:784] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_3\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "input: \"Placeholder/_2\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT32\n",
      "      type: DT_INT32\n",
      "      type: DT_INT32\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 10\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\024TensorSliceDataset:0\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 610\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 610\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"replicate_on_split\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT32\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT32\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT32\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    trainset.shuffle(n).batch(1), epochs=1, batch_size=1,\n",
    "    # validation_data = val_dataset.shuffle(len(X_test['input_ids'])).batch(32),\n",
    "    # callbacks = [callback_earlystop]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperatorNotAllowedInGraphError",
     "evalue": "Exception encountered when calling layer 'longformer' (type TFLongformerMainLayer).\n\nUsing a symbolic `tf.Tensor` as a Python `bool` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.\n\nCall arguments received by layer 'longformer' (type TFLongformerMainLayer):\n  • args=('tf.Tensor(shape=(None, None), dtype=int64)',)\n  • kwargs={'attention_mask': 'tf.Tensor(shape=(None, None), dtype=int64)', 'head_mask': 'None', 'global_attention_mask': 'tf.Tensor(shape=(None, None), dtype=int64)', 'token_type_ids': 'None', 'position_ids': 'None', 'inputs_embeds': 'None', 'output_attentions': 'False', 'output_hidden_states': 'False', 'return_dict': 'True', 'training': 'False'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m            Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/ipykernel_954/1593899960.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lf_1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36mrun_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0munpacked_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_args_and_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munpacked_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;31m# Keras enforces the first layer argument to be passed, and checks it through `inspect.getfullargspec()`. This\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/longformer/modeling_tf_longformer.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, input_ids, attention_mask, head_mask, token_type_ids, position_ids, global_attention_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, labels, training)\u001b[0m\n\u001b[1;32m   2434\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2435\u001b[0m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2436\u001b[0;31m             \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2437\u001b[0m         )\n\u001b[1;32m   2438\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36mrun_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0munpacked_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_args_and_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munpacked_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;31m# Keras enforces the first layer argument to be passed, and checks it through `inspect.getfullargspec()`. This\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/longformer/modeling_tf_longformer.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, input_ids, attention_mask, head_mask, global_attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m   1740\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1742\u001b[0;31m             \u001b[0mpad_token_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1743\u001b[0m         )\n\u001b[1;32m   1744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/longformer/modeling_tf_longformer.py\u001b[0m in \u001b[0;36m_pad_to_window_size\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, pad_token_id)\u001b[0m\n\u001b[1;32m   1820\u001b[0m         \u001b[0mpadding_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattention_window\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mseq_len\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mattention_window\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mattention_window\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1822\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mpadding_len\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1823\u001b[0m             logger.info(\n\u001b[1;32m   1824\u001b[0m                 \u001b[0;34mf\"Input ids are automatically padded from {seq_len} to {seq_len + padding_len} to be a multiple of \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m: Exception encountered when calling layer 'longformer' (type TFLongformerMainLayer).\n\nUsing a symbolic `tf.Tensor` as a Python `bool` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.\n\nCall arguments received by layer 'longformer' (type TFLongformerMainLayer):\n  • args=('tf.Tensor(shape=(None, None), dtype=int64)',)\n  • kwargs={'attention_mask': 'tf.Tensor(shape=(None, None), dtype=int64)', 'head_mask': 'None', 'global_attention_mask': 'tf.Tensor(shape=(None, None), dtype=int64)', 'token_type_ids': 'None', 'position_ids': 'None', 'inputs_embeds': 'None', 'output_attentions': 'False', 'output_hidden_states': 'False', 'return_dict': 'True', 'training': 'False'}"
     ]
    }
   ],
   "source": [
    "# model.save('lf_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_weights('savem/lf_w2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at allenai/longformer-base-4096 were not used when initializing TFLongformerForSequenceClassification: ['lm_head']\n",
      "- This IS expected if you are initializing TFLongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFLongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFLongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "new_model = TFAutoModelForSequenceClassification.from_pretrained(M, num_labels=2, id2label=id2label, label2id=label2id)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001) #learning_rate=5e-5\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n",
    "new_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7fe2ec200810>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.load_weights('savem/lf_w1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextClassificationPipeline\n",
    "\n",
    "# 로드하기\n",
    "\n",
    "text_classifier = TextClassificationPipeline(\n",
    "    tokenizer=tokenizer, \n",
    "    model=model, \n",
    "    framework='tf',\n",
    "    return_all_scores=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_classifier('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    @traceback_utils.filter_traceback\n",
      "    def __call__(self, *args, **kwargs):\n",
      "        if self._layout_map is not None and not self.built:\n",
      "            # Note that this method is only overridden for DTensor and layout\n",
      "            # injection purpose.\n",
      "            # Capture the inputs and create graph input as replacement for model\n",
      "            # to initialize its weights first.\n",
      "            copied_args = copy.copy(args)\n",
      "            copied_kwargs = copy.copy(kwargs)\n",
      "\n",
      "            (\n",
      "                inputs,\n",
      "                copied_args,\n",
      "                copied_kwargs,\n",
      "            ) = self._call_spec.split_out_first_arg(copied_args, copied_kwargs)\n",
      "\n",
      "            def _convert_to_graph_inputs(x):\n",
      "                if isinstance(x, (tf.Tensor, np.ndarray, float, int)):\n",
      "                    x = tf.convert_to_tensor(x)\n",
      "                    return input_layer_module.Input(x.shape)\n",
      "\n",
      "            # TODO(scottzhu): maybe better handle mask and training flag.\n",
      "            inputs = tf.nest.map_structure(_convert_to_graph_inputs, inputs)\n",
      "            copied_args = tf.nest.map_structure(\n",
      "                _convert_to_graph_inputs, copied_args\n",
      "            )\n",
      "            copied_kwargs = tf.nest.map_structure(\n",
      "                _convert_to_graph_inputs, copied_kwargs\n",
      "            )\n",
      "\n",
      "            with layout_map_lib.layout_map_scope(self._layout_map):\n",
      "                # We ignore the result here.\n",
      "                super().__call__(inputs, *copied_args, **copied_kwargs)\n",
      "\n",
      "            layout_map_lib._map_subclass_model_variable(self, self._layout_map)\n",
      "\n",
      "        return super().__call__(*args, **kwargs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(new_model.__call__))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    @traceback_utils.filter_traceback\n",
      "    def fit(\n",
      "        self,\n",
      "        x=None,\n",
      "        y=None,\n",
      "        batch_size=None,\n",
      "        epochs=1,\n",
      "        verbose=\"auto\",\n",
      "        callbacks=None,\n",
      "        validation_split=0.0,\n",
      "        validation_data=None,\n",
      "        shuffle=True,\n",
      "        class_weight=None,\n",
      "        sample_weight=None,\n",
      "        initial_epoch=0,\n",
      "        steps_per_epoch=None,\n",
      "        validation_steps=None,\n",
      "        validation_batch_size=None,\n",
      "        validation_freq=1,\n",
      "        max_queue_size=10,\n",
      "        workers=1,\n",
      "        use_multiprocessing=False,\n",
      "    ):\n",
      "        \"\"\"Trains the model for a fixed number of epochs (iterations on a dataset).\n",
      "\n",
      "        Args:\n",
      "            x: Input data. It could be:\n",
      "              - A Numpy array (or array-like), or a list of arrays\n",
      "                (in case the model has multiple inputs).\n",
      "              - A TensorFlow tensor, or a list of tensors\n",
      "                (in case the model has multiple inputs).\n",
      "              - A dict mapping input names to the corresponding array/tensors,\n",
      "                if the model has named inputs.\n",
      "              - A `tf.data` dataset. Should return a tuple\n",
      "                of either `(inputs, targets)` or\n",
      "                `(inputs, targets, sample_weights)`.\n",
      "              - A generator or `keras.utils.Sequence` returning `(inputs,\n",
      "                targets)` or `(inputs, targets, sample_weights)`.\n",
      "              - A `tf.keras.utils.experimental.DatasetCreator`, which wraps a\n",
      "                callable that takes a single argument of type\n",
      "                `tf.distribute.InputContext`, and returns a `tf.data.Dataset`.\n",
      "                `DatasetCreator` should be used when users prefer to specify the\n",
      "                per-replica batching and sharding logic for the `Dataset`.\n",
      "                See `tf.keras.utils.experimental.DatasetCreator` doc for more\n",
      "                information.\n",
      "              A more detailed description of unpacking behavior for iterator\n",
      "              types (Dataset, generator, Sequence) is given below. If these\n",
      "              include `sample_weights` as a third component, note that sample\n",
      "              weighting applies to the `weighted_metrics` argument but not the\n",
      "              `metrics` argument in `compile()`. If using\n",
      "              `tf.distribute.experimental.ParameterServerStrategy`, only\n",
      "              `DatasetCreator` type is supported for `x`.\n",
      "            y: Target data. Like the input data `x`,\n",
      "              it could be either Numpy array(s) or TensorFlow tensor(s).\n",
      "              It should be consistent with `x` (you cannot have Numpy inputs and\n",
      "              tensor targets, or inversely). If `x` is a dataset, generator,\n",
      "              or `keras.utils.Sequence` instance, `y` should\n",
      "              not be specified (since targets will be obtained from `x`).\n",
      "            batch_size: Integer or `None`.\n",
      "                Number of samples per gradient update.\n",
      "                If unspecified, `batch_size` will default to 32.\n",
      "                Do not specify the `batch_size` if your data is in the\n",
      "                form of datasets, generators, or `keras.utils.Sequence`\n",
      "                instances (since they generate batches).\n",
      "            epochs: Integer. Number of epochs to train the model.\n",
      "                An epoch is an iteration over the entire `x` and `y`\n",
      "                data provided\n",
      "                (unless the `steps_per_epoch` flag is set to\n",
      "                something other than None).\n",
      "                Note that in conjunction with `initial_epoch`,\n",
      "                `epochs` is to be understood as \"final epoch\".\n",
      "                The model is not trained for a number of iterations\n",
      "                given by `epochs`, but merely until the epoch\n",
      "                of index `epochs` is reached.\n",
      "            verbose: 'auto', 0, 1, or 2. Verbosity mode.\n",
      "                0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
      "                'auto' defaults to 1 for most cases, but 2 when used with\n",
      "                `ParameterServerStrategy`. Note that the progress bar is not\n",
      "                particularly useful when logged to a file, so verbose=2 is\n",
      "                recommended when not running interactively (eg, in a production\n",
      "                environment).\n",
      "            callbacks: List of `keras.callbacks.Callback` instances.\n",
      "                List of callbacks to apply during training.\n",
      "                See `tf.keras.callbacks`. Note\n",
      "                `tf.keras.callbacks.ProgbarLogger` and\n",
      "                `tf.keras.callbacks.History` callbacks are created automatically\n",
      "                and need not be passed into `model.fit`.\n",
      "                `tf.keras.callbacks.ProgbarLogger` is created or not based on\n",
      "                `verbose` argument to `model.fit`.\n",
      "                Callbacks with batch-level calls are currently unsupported with\n",
      "                `tf.distribute.experimental.ParameterServerStrategy`, and users\n",
      "                are advised to implement epoch-level calls instead with an\n",
      "                appropriate `steps_per_epoch` value.\n",
      "            validation_split: Float between 0 and 1.\n",
      "                Fraction of the training data to be used as validation data.\n",
      "                The model will set apart this fraction of the training data,\n",
      "                will not train on it, and will evaluate\n",
      "                the loss and any model metrics\n",
      "                on this data at the end of each epoch.\n",
      "                The validation data is selected from the last samples\n",
      "                in the `x` and `y` data provided, before shuffling. This\n",
      "                argument is not supported when `x` is a dataset, generator or\n",
      "                `keras.utils.Sequence` instance.\n",
      "                If both `validation_data` and `validation_split` are provided,\n",
      "                `validation_data` will override `validation_split`.\n",
      "                `validation_split` is not yet supported with\n",
      "                `tf.distribute.experimental.ParameterServerStrategy`.\n",
      "            validation_data: Data on which to evaluate\n",
      "                the loss and any model metrics at the end of each epoch.\n",
      "                The model will not be trained on this data. Thus, note the fact\n",
      "                that the validation loss of data provided using\n",
      "                `validation_split` or `validation_data` is not affected by\n",
      "                regularization layers like noise and dropout.\n",
      "                `validation_data` will override `validation_split`.\n",
      "                `validation_data` could be:\n",
      "                  - A tuple `(x_val, y_val)` of Numpy arrays or tensors.\n",
      "                  - A tuple `(x_val, y_val, val_sample_weights)` of NumPy\n",
      "                    arrays.\n",
      "                  - A `tf.data.Dataset`.\n",
      "                  - A Python generator or `keras.utils.Sequence` returning\n",
      "                  `(inputs, targets)` or `(inputs, targets, sample_weights)`.\n",
      "                `validation_data` is not yet supported with\n",
      "                `tf.distribute.experimental.ParameterServerStrategy`.\n",
      "            shuffle: Boolean (whether to shuffle the training data\n",
      "                before each epoch) or str (for 'batch'). This argument is\n",
      "                ignored when `x` is a generator or an object of tf.data.Dataset.\n",
      "                'batch' is a special option for dealing\n",
      "                with the limitations of HDF5 data; it shuffles in batch-sized\n",
      "                chunks. Has no effect when `steps_per_epoch` is not `None`.\n",
      "            class_weight: Optional dictionary mapping class indices (integers)\n",
      "                to a weight (float) value, used for weighting the loss function\n",
      "                (during training only).\n",
      "                This can be useful to tell the model to\n",
      "                \"pay more attention\" to samples from\n",
      "                an under-represented class.\n",
      "            sample_weight: Optional Numpy array of weights for\n",
      "                the training samples, used for weighting the loss function\n",
      "                (during training only). You can either pass a flat (1D)\n",
      "                Numpy array with the same length as the input samples\n",
      "                (1:1 mapping between weights and samples),\n",
      "                or in the case of temporal data,\n",
      "                you can pass a 2D array with shape\n",
      "                `(samples, sequence_length)`,\n",
      "                to apply a different weight to every timestep of every sample.\n",
      "                This argument is not supported when `x` is a dataset, generator,\n",
      "                or `keras.utils.Sequence` instance, instead provide the\n",
      "                sample_weights as the third element of `x`.\n",
      "                Note that sample weighting does not apply to metrics specified\n",
      "                via the `metrics` argument in `compile()`. To apply sample\n",
      "                weighting to your metrics, you can specify them via the\n",
      "                `weighted_metrics` in `compile()` instead.\n",
      "            initial_epoch: Integer.\n",
      "                Epoch at which to start training\n",
      "                (useful for resuming a previous training run).\n",
      "            steps_per_epoch: Integer or `None`.\n",
      "                Total number of steps (batches of samples)\n",
      "                before declaring one epoch finished and starting the\n",
      "                next epoch. When training with input tensors such as\n",
      "                TensorFlow data tensors, the default `None` is equal to\n",
      "                the number of samples in your dataset divided by\n",
      "                the batch size, or 1 if that cannot be determined. If x is a\n",
      "                `tf.data` dataset, and 'steps_per_epoch'\n",
      "                is None, the epoch will run until the input dataset is\n",
      "                exhausted.  When passing an infinitely repeating dataset, you\n",
      "                must specify the `steps_per_epoch` argument. If\n",
      "                `steps_per_epoch=-1` the training will run indefinitely with an\n",
      "                infinitely repeating dataset.  This argument is not supported\n",
      "                with array inputs.\n",
      "                When using `tf.distribute.experimental.ParameterServerStrategy`:\n",
      "                  * `steps_per_epoch=None` is not supported.\n",
      "            validation_steps: Only relevant if `validation_data` is provided and\n",
      "                is a `tf.data` dataset. Total number of steps (batches of\n",
      "                samples) to draw before stopping when performing validation\n",
      "                at the end of every epoch. If 'validation_steps' is None,\n",
      "                validation will run until the `validation_data` dataset is\n",
      "                exhausted. In the case of an infinitely repeated dataset, it\n",
      "                will run into an infinite loop. If 'validation_steps' is\n",
      "                specified and only part of the dataset will be consumed, the\n",
      "                evaluation will start from the beginning of the dataset at each\n",
      "                epoch. This ensures that the same validation samples are used\n",
      "                every time.\n",
      "            validation_batch_size: Integer or `None`.\n",
      "                Number of samples per validation batch.\n",
      "                If unspecified, will default to `batch_size`.\n",
      "                Do not specify the `validation_batch_size` if your data is in\n",
      "                the form of datasets, generators, or `keras.utils.Sequence`\n",
      "                instances (since they generate batches).\n",
      "            validation_freq: Only relevant if validation data is provided.\n",
      "              Integer or `collections.abc.Container` instance (e.g. list, tuple,\n",
      "              etc.).  If an integer, specifies how many training epochs to run\n",
      "              before a new validation run is performed, e.g. `validation_freq=2`\n",
      "              runs validation every 2 epochs. If a Container, specifies the\n",
      "              epochs on which to run validation, e.g.\n",
      "              `validation_freq=[1, 2, 10]` runs validation at the end of the\n",
      "              1st, 2nd, and 10th epochs.\n",
      "            max_queue_size: Integer. Used for generator or\n",
      "              `keras.utils.Sequence` input only. Maximum size for the generator\n",
      "              queue.  If unspecified, `max_queue_size` will default to 10.\n",
      "            workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
      "                only. Maximum number of processes to spin up\n",
      "                when using process-based threading. If unspecified, `workers`\n",
      "                will default to 1.\n",
      "            use_multiprocessing: Boolean. Used for generator or\n",
      "                `keras.utils.Sequence` input only. If `True`, use process-based\n",
      "                threading. If unspecified, `use_multiprocessing` will default to\n",
      "                `False`. Note that because this implementation relies on\n",
      "                multiprocessing, you should not pass non-picklable arguments to\n",
      "                the generator as they can't be passed easily to children\n",
      "                processes.\n",
      "\n",
      "        Unpacking behavior for iterator-like inputs:\n",
      "            A common pattern is to pass a tf.data.Dataset, generator, or\n",
      "          tf.keras.utils.Sequence to the `x` argument of fit, which will in fact\n",
      "          yield not only features (x) but optionally targets (y) and sample\n",
      "          weights.  Keras requires that the output of such iterator-likes be\n",
      "          unambiguous. The iterator should return a tuple of length 1, 2, or 3,\n",
      "          where the optional second and third elements will be used for y and\n",
      "          sample_weight respectively. Any other type provided will be wrapped in\n",
      "          a length one tuple, effectively treating everything as 'x'. When\n",
      "          yielding dicts, they should still adhere to the top-level tuple\n",
      "          structure.\n",
      "          e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate\n",
      "          features, targets, and weights from the keys of a single dict.\n",
      "            A notable unsupported data type is the namedtuple. The reason is\n",
      "          that it behaves like both an ordered datatype (tuple) and a mapping\n",
      "          datatype (dict). So given a namedtuple of the form:\n",
      "              `namedtuple(\"example_tuple\", [\"y\", \"x\"])`\n",
      "          it is ambiguous whether to reverse the order of the elements when\n",
      "          interpreting the value. Even worse is a tuple of the form:\n",
      "              `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])`\n",
      "          where it is unclear if the tuple was intended to be unpacked into x,\n",
      "          y, and sample_weight or passed through as a single element to `x`. As\n",
      "          a result the data processing code will simply raise a ValueError if it\n",
      "          encounters a namedtuple. (Along with instructions to remedy the\n",
      "          issue.)\n",
      "\n",
      "        Returns:\n",
      "            A `History` object. Its `History.history` attribute is\n",
      "            a record of training loss values and metrics values\n",
      "            at successive epochs, as well as validation loss values\n",
      "            and validation metrics values (if applicable).\n",
      "\n",
      "        Raises:\n",
      "            RuntimeError: 1. If the model was never compiled or,\n",
      "            2. If `model.fit` is  wrapped in `tf.function`.\n",
      "\n",
      "            ValueError: In case of mismatch between the provided input data\n",
      "                and what the model expects or when the input data is empty.\n",
      "        \"\"\"\n",
      "        base_layer.keras_api_gauge.get_cell(\"fit\").set(True)\n",
      "        # Legacy graph support is contained in `training_v1.Model`.\n",
      "        version_utils.disallow_legacy_graph(\"Model\", \"fit\")\n",
      "        self._assert_compile_was_called()\n",
      "        self._check_call_args(\"fit\")\n",
      "        _disallow_inside_tf_function(\"fit\")\n",
      "\n",
      "        verbose = _get_verbosity(verbose, self.distribute_strategy)\n",
      "\n",
      "        if validation_split and validation_data is None:\n",
      "            # Create the validation data using the training data. Only supported\n",
      "            # for `Tensor` and `NumPy` input.\n",
      "            (\n",
      "                x,\n",
      "                y,\n",
      "                sample_weight,\n",
      "            ), validation_data = data_adapter.train_validation_split(\n",
      "                (x, y, sample_weight), validation_split=validation_split\n",
      "            )\n",
      "\n",
      "        if validation_data:\n",
      "            (\n",
      "                val_x,\n",
      "                val_y,\n",
      "                val_sample_weight,\n",
      "            ) = data_adapter.unpack_x_y_sample_weight(validation_data)\n",
      "\n",
      "        if self.distribute_strategy._should_use_with_coordinator:\n",
      "            self._cluster_coordinator = (\n",
      "                tf.distribute.experimental.coordinator.ClusterCoordinator(\n",
      "                    self.distribute_strategy\n",
      "                )\n",
      "            )\n",
      "\n",
      "        with self.distribute_strategy.scope(), training_utils.RespectCompiledTrainableState(  # noqa: E501\n",
      "            self\n",
      "        ):\n",
      "            # Creates a `tf.data.Dataset` and handles batch and epoch iteration.\n",
      "            data_handler = data_adapter.get_data_handler(\n",
      "                x=x,\n",
      "                y=y,\n",
      "                sample_weight=sample_weight,\n",
      "                batch_size=batch_size,\n",
      "                steps_per_epoch=steps_per_epoch,\n",
      "                initial_epoch=initial_epoch,\n",
      "                epochs=epochs,\n",
      "                shuffle=shuffle,\n",
      "                class_weight=class_weight,\n",
      "                max_queue_size=max_queue_size,\n",
      "                workers=workers,\n",
      "                use_multiprocessing=use_multiprocessing,\n",
      "                model=self,\n",
      "                steps_per_execution=self._steps_per_execution,\n",
      "            )\n",
      "\n",
      "            # Container that configures and calls `tf.keras.Callback`s.\n",
      "            if not isinstance(callbacks, callbacks_module.CallbackList):\n",
      "                callbacks = callbacks_module.CallbackList(\n",
      "                    callbacks,\n",
      "                    add_history=True,\n",
      "                    add_progbar=verbose != 0,\n",
      "                    model=self,\n",
      "                    verbose=verbose,\n",
      "                    epochs=epochs,\n",
      "                    steps=data_handler.inferred_steps,\n",
      "                )\n",
      "\n",
      "            self.stop_training = False\n",
      "            self.train_function = self.make_train_function()\n",
      "            self._train_counter.assign(0)\n",
      "            callbacks.on_train_begin()\n",
      "            training_logs = None\n",
      "            # Handle fault-tolerance for multi-worker.\n",
      "            # TODO(omalleyt): Fix the ordering issues that mean this has to\n",
      "            # happen after `callbacks.on_train_begin`.\n",
      "            steps_per_epoch_inferred = (\n",
      "                steps_per_epoch or data_handler.inferred_steps\n",
      "            )\n",
      "            (\n",
      "                data_handler._initial_epoch,\n",
      "                data_handler._initial_step,\n",
      "            ) = self._maybe_load_initial_counters_from_ckpt(\n",
      "                steps_per_epoch_inferred, initial_epoch\n",
      "            )\n",
      "            logs = None\n",
      "            for epoch, iterator in data_handler.enumerate_epochs():\n",
      "                self.reset_metrics()\n",
      "                callbacks.on_epoch_begin(epoch)\n",
      "                with data_handler.catch_stop_iteration():\n",
      "                    for step in data_handler.steps():\n",
      "                        with tf.profiler.experimental.Trace(\n",
      "                            \"train\",\n",
      "                            epoch_num=epoch,\n",
      "                            step_num=step,\n",
      "                            batch_size=batch_size,\n",
      "                            _r=1,\n",
      "                        ):\n",
      "                            callbacks.on_train_batch_begin(step)\n",
      "                            tmp_logs = self.train_function(iterator)\n",
      "                            if data_handler.should_sync:\n",
      "                                context.async_wait()\n",
      "                            # No error, now safe to assign to logs.\n",
      "                            logs = tmp_logs\n",
      "                            end_step = step + data_handler.step_increment\n",
      "                            callbacks.on_train_batch_end(end_step, logs)\n",
      "                            if self.stop_training:\n",
      "                                break\n",
      "\n",
      "                logs = tf_utils.sync_to_numpy_or_python_type(logs)\n",
      "                if logs is None:\n",
      "                    raise ValueError(\n",
      "                        \"Unexpected result of `train_function` \"\n",
      "                        \"(Empty logs). Please use \"\n",
      "                        \"`Model.compile(..., run_eagerly=True)`, or \"\n",
      "                        \"`tf.config.run_functions_eagerly(True)` for more \"\n",
      "                        \"information of where went wrong, or file a \"\n",
      "                        \"issue/bug to `tf.keras`.\"\n",
      "                    )\n",
      "                # Override with model metrics instead of last step logs\n",
      "                logs = self._validate_and_get_metrics_result(logs)\n",
      "                epoch_logs = copy.copy(logs)\n",
      "\n",
      "                # Run validation.\n",
      "                if validation_data and self._should_eval(\n",
      "                    epoch, validation_freq\n",
      "                ):\n",
      "                    # Create data_handler for evaluation and cache it.\n",
      "                    if getattr(self, \"_eval_data_handler\", None) is None:\n",
      "                        self._eval_data_handler = data_adapter.get_data_handler(\n",
      "                            x=val_x,\n",
      "                            y=val_y,\n",
      "                            sample_weight=val_sample_weight,\n",
      "                            batch_size=validation_batch_size or batch_size,\n",
      "                            steps_per_epoch=validation_steps,\n",
      "                            initial_epoch=0,\n",
      "                            epochs=1,\n",
      "                            max_queue_size=max_queue_size,\n",
      "                            workers=workers,\n",
      "                            use_multiprocessing=use_multiprocessing,\n",
      "                            model=self,\n",
      "                            steps_per_execution=self._steps_per_execution,\n",
      "                        )\n",
      "                    val_logs = self.evaluate(\n",
      "                        x=val_x,\n",
      "                        y=val_y,\n",
      "                        sample_weight=val_sample_weight,\n",
      "                        batch_size=validation_batch_size or batch_size,\n",
      "                        steps=validation_steps,\n",
      "                        callbacks=callbacks,\n",
      "                        max_queue_size=max_queue_size,\n",
      "                        workers=workers,\n",
      "                        use_multiprocessing=use_multiprocessing,\n",
      "                        return_dict=True,\n",
      "                        _use_cached_eval_dataset=True,\n",
      "                    )\n",
      "                    val_logs = {\n",
      "                        \"val_\" + name: val for name, val in val_logs.items()\n",
      "                    }\n",
      "                    epoch_logs.update(val_logs)\n",
      "\n",
      "                callbacks.on_epoch_end(epoch, epoch_logs)\n",
      "                training_logs = epoch_logs\n",
      "                if self.stop_training:\n",
      "                    break\n",
      "\n",
      "            if (\n",
      "                isinstance(self.optimizer, optimizer_experimental.Optimizer)\n",
      "                and epochs > 0\n",
      "            ):\n",
      "                self.optimizer.finalize_variable_values(\n",
      "                    self.trainable_variables\n",
      "                )\n",
      "\n",
      "            # If eval data_handler exists, delete it after all epochs are done.\n",
      "            if getattr(self, \"_eval_data_handler\", None) is not None:\n",
      "                del self._eval_data_handler\n",
      "            callbacks.on_train_end(logs=training_logs)\n",
      "            return self.history\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(new_model.fit))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-14 05:02:55.748295: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1096531968 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "a = tf.convert_to_tensor(X_train['input_ids'])\n",
    "del a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 217s 2s/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = new_model.predict(a[:1000], batch_size=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_score = [np.argmax(e) for e in y_pred[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_loss, test_acc = new_model.evaluate(X_train['input_ids'],y_train verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "\n",
    "f1 = f1_score(y_train[:1000],y_pred_score)\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFLongformerSequenceClassifierOutput(loss=None, logits=array([[ 1.9855249, -1.9520394],\n",
       "       [ 1.985525 , -1.9520394],\n",
       "       [ 1.9855248, -1.9520394],\n",
       "       ...,\n",
       "       [ 1.9855248, -1.9520395],\n",
       "       [ 1.9855248, -1.9520394],\n",
       "       [ 1.9855248, -1.9520394]], dtype=float32), hidden_states=None, attentions=None, global_attentions=None)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          NG       0.97      1.00      0.98       970\n",
      "           G       0.00      0.00      0.00        30\n",
      "\n",
      "    accuracy                           0.97      1000\n",
      "   macro avg       0.48      0.50      0.49      1000\n",
      "weighted avg       0.94      0.97      0.96      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train[:1000], y_pred_score, target_names=['NG','G']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
